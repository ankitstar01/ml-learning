{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poetry_seq_to_seq.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2cI002fYG7QL7mOffeia4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitstar01/ml-learning/blob/master/poetry_seq_to_seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7Nlto5T4hQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense,Embedding,LSTM,Input\n",
        "from keras.optimizers import Adam,SGD\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "MAX_SEQ_LEN=100\n",
        "MAX_VOCAB_SIZE=3000\n",
        "EMBEDDING_DIM=50\n",
        "VALIDATION_SPLIT=0.2\n",
        "BATCH_SIZE=128\n",
        "EPOCHS=2000\n",
        "LATEN_DIM=25"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asAUotvR4lWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#loading the data\n",
        "input_data=[]\n",
        "output_data=[]\n",
        "\n",
        "for line in open('./robert_frost.txt'):\n",
        "    line=line.rstrip()\n",
        "    if not line:\n",
        "        continue\n",
        "    \n",
        "    input_line='<sos>'+line\n",
        "    output_line=line+'<eos>'\n",
        "    \n",
        "    input_data.append(input_line)\n",
        "    output_data.append(output_line)\n",
        "all_line=input_data+output_data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNS8hijH46aA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b910acb1-8be3-4fe9-b066-7c50bbcbd794"
      },
      "source": [
        "tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE,filters='')\n",
        "tokenizer.fit_on_texts(all_line)\n",
        "input_seq=tokenizer.texts_to_sequences(input_data)\n",
        "output_seq=tokenizer.texts_to_sequences(output_data)\n",
        "max_seq_len=max(len(s) for s in input_seq)\n",
        "print('Max Seq Len {}'.format(max_seq_len))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Seq Len 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq0LEmYu48FL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "25fbc0e1-2e7b-4164-ff7e-037244c38389"
      },
      "source": [
        "#word to index mapping\n",
        "word2idx=tokenizer.word_index\n",
        "print('total unique tokens %s '%len(word2idx))\n",
        "# assert('<sos>' in word2idx)\n",
        "# assert('<eos>' in word2idx)\n",
        "\n",
        "max_seq_len=min(MAX_SEQ_LEN,max_seq_len)\n",
        "input_seq=pad_sequences(input_seq,maxlen=max_seq_len,padding='post')\n",
        "output_seq=pad_sequences(output_seq,maxlen=max_seq_len,padding='post')\n",
        "print('shape of input tensor : ',input_seq.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total unique tokens 4614 \n",
            "shape of input tensor :  (1436, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPzjFZvG5NEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "c9cb99a1-b3dd-4615-fe23-2cf2a0941939"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = 'http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip'\n",
        "r = requests.get(url, stream=True)\n",
        "open('glove.zip', 'wb').write(r.content)\n",
        "!unzip ./glove.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./glove.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUwjvPFb6DVI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "fc7c3b7a-b8db-419b-9285-32f4c2957e1e"
      },
      "source": [
        "print('loading word vector')\n",
        "word2vec={}\n",
        "with open(os.path.join('./glove.6B.%sd.txt'%EMBIDDING_DIM)) as f:\n",
        "    for line in f:\n",
        "        value=line.split()\n",
        "        word=value[0]\n",
        "        vec=np.asarray(value[1:],dtype='float32')\n",
        "        word2vec[word]=vec\n",
        "print('found %s word vectors'%len(word2vec))\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading word vector\n",
            "found 400000 word vectors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuPMKw-57d5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdc4af4e-2ba9-4464-a584-30087312ac77"
      },
      "source": [
        "print('filling pretrained embedding')\n",
        "num_words=min(MAX_VOCAB_SIZE,len(word2idx)+1)\n",
        "embedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\n",
        "for word,i in word2idx.items():\n",
        "  if i<MAX_VOCAB_SIZE:\n",
        "    embedding_vec=word2vec.get(word)\n",
        "    if embedding_vec is not None:\n",
        "      embedding_matrix[i]=embedding_vec"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filling pretrained embedding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOFkWm8vhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}